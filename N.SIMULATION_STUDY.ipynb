{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "from econml.metalearners import TLearner, SLearner, XLearner\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cohort  gender  hh_size  edu_level  baseline_income   \n",
      "0  26-35       0        6  18.341193      1652.768920  \\\n",
      "1  56-66       0        2  16.273134      1891.797345   \n",
      "2  36-45       0        4  17.212297      1803.524148   \n",
      "3  36-45       1        7  17.874516      1493.089204   \n",
      "4   1-25       0       11  19.213854       618.861966   \n",
      "\n",
      "   baseline_labor_force_status  W       income  \n",
      "0                            0  0  1652.768920  \n",
      "1                            0  1  1656.043264  \n",
      "2                            1  1  2333.338937  \n",
      "3                            1  1   335.242330  \n",
      "4                            1  1  3342.256755  \n"
     ]
    }
   ],
   "source": [
    "# Data generation process- Simulation 1- linear and non linear terms in treatment effect- 80:20 treatment assignment- linear and non linera terms in baseline outcome\n",
    "\n",
    "# Parameters\n",
    "n = 10000  # Number of observations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Covariates\n",
    "# Age groups corresponding to cohorts\n",
    "age_groups = np.random.choice([1, 2, 3, 4, 5], size=n, p=[0.2, 0.3, 0.3, 0.1, 0.1])\n",
    "age_dict = {1: '1-25', 2: '26-35', 3: '36-45', 4: '46-55', 5: '56-66'}\n",
    "cohort = np.array([age_dict[age] for age in age_groups])\n",
    "\n",
    "gender = np.random.binomial(1, 0.5, n)\n",
    "hh_size = np.random.randint(1, 13, n)  # Household size varies from 1 to 12\n",
    "edu_level = np.random.normal(16, 2, n)\n",
    "\n",
    "# Covariate matrix (excluding cohort since it is categorical now)\n",
    "X = np.column_stack((age_groups, gender, hh_size, edu_level))\n",
    "\n",
    "# Desired proportion of treatment group\n",
    "treatment_proportion = 0.8  # 80% treatment, 20% control\n",
    "\n",
    "# Assign treatment based on a probability distribution\n",
    "W = np.random.binomial(1, treatment_proportion, n)\n",
    "\n",
    "# Generate baseline income\n",
    "baseline_income = (1500 \n",
    "                   + 20 * X[:, 0]  # Linear term for age group (cohort)\n",
    "                   + 50 * X[:, 1]  # Linear effect for gender\n",
    "                   - 10 * X[:, 2]**2  # quadratic term for hh_size\n",
    "                   + 25 * X[:, 3]  # Linear effect for edu_level\n",
    "                   + np.random.normal(0, 100, n))  # Add some noise\n",
    "\n",
    "# Generate baseline labor force status\n",
    "baseline_labor_force_status = np.random.binomial(1, 0.7, n)  # 70% in labor force\n",
    "\n",
    "# Non-linear Baseline Outcome for Income\n",
    "income_0 = baseline_income\n",
    "# Linear and Non-linear Treatment Effect\n",
    "tau_income = (300 \n",
    "              + 50 * (X[:, 2])  # Linear effect for household size\n",
    "              + 20 * edu_level  # linear term\n",
    "              + 10 * np.sin(edu_level)  # non-linear term\n",
    "              + np.random.normal(0, 1500, n))  # Adding moderate noise\n",
    "\n",
    "# Treated outcome\n",
    "income_1 = income_0 + tau_income\n",
    "\n",
    "# Observed outcome\n",
    "income = W * income_1 + (1 - W) * income_0\n",
    "\n",
    "# Combine data into a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'cohort': cohort,\n",
    "    'gender': gender,\n",
    "    'hh_size': hh_size,\n",
    "    'edu_level': edu_level,\n",
    "    'baseline_income': baseline_income,\n",
    "    'baseline_labor_force_status': baseline_labor_force_status,\n",
    "    'W': W,\n",
    "    'income': income\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Average Bias: -0.7904896577587616\n",
      "Average MSE: 15536.79811099645\n",
      "Average RMSE: 124.64669314103945\n",
      "Average MAE: 99.57871754046364\n",
      "R-squared: 0.6168494995161107\n",
      "Explained Variance: 0.6184034536938602\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Average Bias: 0.530583765381042\n",
      "Average MSE: 10156.924957114357\n",
      "Average RMSE: 100.78157052315844\n",
      "Average MAE: 80.3387450804248\n",
      "R-squared: 0.7492932301314995\n",
      "Explained Variance: 0.7505090655247415\n"
     ]
    }
   ],
   "source": [
    "# simulation 1 with random forest and gradient boosting regressors\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((cohort_encoded, data[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "W_data = data['W'].values\n",
    "income_0_data = income_0\n",
    "income_1_data = income_1\n",
    "true_tau = tau_income\n",
    "\n",
    "# Define a function to perform a single iteration for Random Forest\n",
    "def single_iteration_rf(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Random Forest models\n",
    "    outcome_model_control = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    outcome_model_treated = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    # Calculate metrics\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Define a function to perform a single iteration for Gradient Boosting\n",
    "def single_iteration_gb(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Gradient Boosting models\n",
    "    outcome_model_control = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10, learning_rate=0.05)\n",
    "    outcome_model_treated = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10, learning_rate=0.05)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    # Calculate metrics\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Parallelize the computation across multiple cores for Random Forest\n",
    "results_rf = Parallel(n_jobs=-1)(delayed(single_iteration_rf)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Random Forest\n",
    "avg_bias_rf = np.mean([result[0] for result in results_rf])\n",
    "avg_mse_rf = np.mean([result[1] for result in results_rf])\n",
    "avg_mae_rf = np.mean([result[2] for result in results_rf])\n",
    "avg_r_squared_rf = np.mean([result[3] for result in results_rf])\n",
    "avg_explained_variance_rf = np.mean([result[4] for result in results_rf])\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Average Bias: {avg_bias_rf}\")\n",
    "print(f\"Average MSE: {avg_mse_rf}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_rf)}\")\n",
    "print(f\"Average MAE: {avg_mae_rf}\")\n",
    "print(f\"R-squared: {avg_r_squared_rf}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_rf}\")\n",
    "\n",
    "# Parallelize the computation across multiple cores for Gradient Boosting\n",
    "results_gb = Parallel(n_jobs=-1)(delayed(single_iteration_gb)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Gradient Boosting\n",
    "avg_bias_gb = np.mean([result[0] for result in results_gb])\n",
    "avg_mse_gb = np.mean([result[1] for result in results_gb])\n",
    "avg_mae_gb = np.mean([result[2] for result in results_gb])\n",
    "avg_r_squared_gb = np.mean([result[3] for result in results_gb])\n",
    "avg_explained_variance_gb = np.mean([result[4] for result in results_gb])\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(f\"Average Bias: {avg_bias_gb}\")\n",
    "print(f\"Average MSE: {avg_mse_gb}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_gb)}\")\n",
    "print(f\"Average MAE: {avg_mae_gb}\")\n",
    "print(f\"R-squared: {avg_r_squared_gb}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cohort  gender  hh_size  edu_level  baseline_income   \n",
      "0  26-35       0        6  18.341193      1652.768920  \\\n",
      "1  56-66       0        2  16.273134      1891.797345   \n",
      "2  36-45       0        4  17.212297      1803.524148   \n",
      "3  36-45       1        7  17.874516      1493.089204   \n",
      "4   1-25       0       11  19.213854       618.861966   \n",
      "\n",
      "   baseline_labor_force_status  W       income  \n",
      "0                            0  1  2307.901716  \n",
      "1                            0  0  1891.797345  \n",
      "2                            1  1  2333.338937  \n",
      "3                            1  0  1493.089204  \n",
      "4                            1  0   618.861966  \n"
     ]
    }
   ],
   "source": [
    "# DGP for simulation 2 with 50-50 split in treatment assignment\n",
    "\n",
    "# Parameters\n",
    "n = 10000  # Number of observations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Covariates\n",
    "# Age groups corresponding to cohorts\n",
    "age_groups = np.random.choice([1, 2, 3, 4, 5], size=n, p=[0.2, 0.3, 0.3, 0.1, 0.1])\n",
    "age_dict = {1: '1-25', 2: '26-35', 3: '36-45', 4: '46-55', 5: '56-66'}\n",
    "cohort = np.array([age_dict[age] for age in age_groups])\n",
    "\n",
    "gender = np.random.binomial(1, 0.5, n)\n",
    "hh_size = np.random.randint(1, 13, n)  # Household size varies from 1 to 12\n",
    "edu_level = np.random.normal(16, 2, n)\n",
    "\n",
    "# Covariate matrix (excluding cohort since it is categorical now)\n",
    "X = np.column_stack((age_groups, gender, hh_size, edu_level))\n",
    "\n",
    "# Desired proportion of treatment group\n",
    "treatment_proportion = 0.5  # 50% treatment, 50% control\n",
    "\n",
    "# Assign treatment based on a 50/50 probability distribution\n",
    "W = np.random.binomial(1, treatment_proportion, n)\n",
    "\n",
    "# Generate baseline income\n",
    "baseline_income = (1500 \n",
    "                   + 20 * X[:, 0]  # Linear term for age group (cohort)\n",
    "                   + 50 * X[:, 1]  # Linear effect for gender\n",
    "                   - 10 * X[:, 2]**2  # quadratic term for hh_size\n",
    "                   + 25 * X[:, 3]  # Linear effect for edu_level\n",
    "                   + np.random.normal(0, 100, n))  # Add some noise\n",
    "\n",
    "# Generate baseline labor force status\n",
    "baseline_labor_force_status = np.random.binomial(1, 0.7, n)  # 70% in labor force\n",
    "\n",
    "# Non-linear Baseline Outcome for Income\n",
    "income_0 = baseline_income\n",
    "\n",
    "# Linear and Non-linear Treatment Effect\n",
    "tau_income = (300 \n",
    "              + 50 * (X[:, 2])  # Linear effect for household size\n",
    "              + 20 * edu_level  # linear term\n",
    "              + 10 * np.sin(edu_level)  # non-linear term\n",
    "              + np.random.normal(0, 1500, n))  # Adding moderate noise\n",
    "\n",
    "# Treated outcome\n",
    "income_1 = income_0 + tau_income\n",
    "\n",
    "# Observed outcome\n",
    "income = W * income_1 + (1 - W) * income_0\n",
    "\n",
    "# Combine data into a DataFrame and rename it to data_sim1\n",
    "data_sim1 = pd.DataFrame({\n",
    "    'cohort': cohort,\n",
    "    'gender': gender,\n",
    "    'hh_size': hh_size,\n",
    "    'edu_level': edu_level,\n",
    "    'baseline_income': baseline_income,\n",
    "    'baseline_labor_force_status': baseline_labor_force_status,\n",
    "    'W': W,\n",
    "    'income': income\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data_sim1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Average Bias: -0.9289001931183265\n",
      "Average MSE: 2156765.9043037686\n",
      "Average RMSE: 1468.593171815724\n",
      "Average MAE: 1170.1318691605497\n",
      "R-squared: 0.04675472792317272\n",
      "Explained Variance: 0.05120777992062706\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Average Bias: -2.7131265037175787\n",
      "Average MSE: 1965815.0335941978\n",
      "Average RMSE: 1402.0752596042046\n",
      "Average MAE: 1074.0084055259126\n",
      "R-squared: 0.1319616004288339\n",
      "Explained Variance: 0.1375338586991416\n"
     ]
    }
   ],
   "source": [
    "# SIMULATION 2 results with Random Forest and Gradient Boosting Regressors\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data_sim1[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((cohort_encoded, data_sim1[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "W_data = data_sim1['W'].values\n",
    "income_0_data = income_0\n",
    "income_1_data = income_1\n",
    "true_tau = tau_income\n",
    "\n",
    "# Define a function to perform a single iteration for Random Forest\n",
    "def single_iteration_rf(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Random Forest models\n",
    "    outcome_model_control = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    outcome_model_treated = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Define a function to perform a single iteration for Gradient Boosting\n",
    "def single_iteration_gb(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Gradient Boosting models\n",
    "    outcome_model_control = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    outcome_model_treated = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Parallelize the computation across multiple cores for both Random Forest and Gradient Boosting\n",
    "results_rf = Parallel(n_jobs=-1)(delayed(single_iteration_rf)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "results_gb = Parallel(n_jobs=-1)(delayed(single_iteration_gb)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Random Forest\n",
    "avg_bias_rf = np.mean([result[0] for result in results_rf])\n",
    "avg_mse_rf = np.mean([result[1] for result in results_rf])\n",
    "avg_mae_rf = np.mean([result[2] for result in results_rf])\n",
    "avg_r_squared_rf = np.mean([result[3] for result in results_rf])\n",
    "avg_explained_variance_rf = np.mean([result[4] for result in results_rf])\n",
    "\n",
    "# Aggregate results for Gradient Boosting\n",
    "avg_bias_gb = np.mean([result[0] for result in results_gb])\n",
    "avg_mse_gb = np.mean([result[1] for result in results_gb])\n",
    "avg_mae_gb = np.mean([result[2] for result in results_gb])\n",
    "avg_r_squared_gb = np.mean([result[3] for result in results_gb])\n",
    "avg_explained_variance_gb = np.mean([result[4] for result in results_gb])\n",
    "\n",
    "# Print the results in the desired format\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Average Bias: {avg_bias_rf}\")\n",
    "print(f\"Average MSE: {avg_mse_rf}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_rf)}\")\n",
    "print(f\"Average MAE: {avg_mae_rf}\")\n",
    "print(f\"R-squared: {avg_r_squared_rf}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_rf}\")\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(f\"Average Bias: {avg_bias_gb}\")\n",
    "print(f\"Average MSE: {avg_mse_gb}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_gb)}\")\n",
    "print(f\"Average MAE: {avg_mae_gb}\")\n",
    "print(f\"R-squared: {avg_r_squared_gb}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cohort  gender  hh_size  edu_level  baseline_income   \n",
      "0  26-35       0        6  18.341193      1652.768920  \\\n",
      "1  56-66       0        2  16.273134      1891.797345   \n",
      "2  36-45       0        4  17.212297      1803.524148   \n",
      "3  36-45       1        7  17.874516      1493.089204   \n",
      "4   1-25       0       11  19.213854       618.861966   \n",
      "\n",
      "   baseline_labor_force_status  W       income  \n",
      "0                            0  0  1652.768920  \n",
      "1                            0  1  1765.631238  \n",
      "2                            1  1  2482.653655  \n",
      "3                            1  1   649.557048  \n",
      "4                            1  1  3981.914114  \n"
     ]
    }
   ],
   "source": [
    "# DGP FOR SIMULATION 3 WITH MORE COMPLEXITY IN TREATMENT EFFECT\n",
    "\n",
    "# Parameters\n",
    "n = 10000  # Number of observations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Covariates\n",
    "# Age groups corresponding to cohorts\n",
    "age_groups = np.random.choice([1, 2, 3, 4, 5], size=n, p=[0.2, 0.3, 0.3, 0.1, 0.1])\n",
    "age_dict = {1: '1-25', 2: '26-35', 3: '36-45', 4: '46-55', 5: '56-66'}\n",
    "cohort = np.array([age_dict[age] for age in age_groups])\n",
    "\n",
    "gender = np.random.binomial(1, 0.5, n)\n",
    "hh_size = np.random.randint(1, 13, n)  # Household size varies from 1 to 12\n",
    "edu_level = np.random.normal(16, 2, n)\n",
    "\n",
    "# Covariate matrix (excluding cohort since it is categorical now)\n",
    "X = np.column_stack((age_groups, gender, hh_size, edu_level))\n",
    "\n",
    "# Desired proportion of treatment group\n",
    "treatment_proportion = 0.8  # 80% treatment, 20% control\n",
    "\n",
    "# Assign treatment based on a 80/20 probability distribution\n",
    "W = np.random.binomial(1, treatment_proportion, n)\n",
    "\n",
    "# Generate baseline income\n",
    "baseline_income = (1500 \n",
    "                   + 20 * X[:, 0]  # Linear term for age group (cohort)\n",
    "                   + 50 * X[:, 1]  # Linear effect for gender\n",
    "                   - 10 * X[:, 2]**2  # quadratic term for hh_size\n",
    "                   + 25 * X[:, 3]  # Linear effect for edu_level\n",
    "                   + np.random.normal(0, 100, n))  # Add some noise\n",
    "\n",
    "# Generate baseline labor force status\n",
    "baseline_labor_force_status = np.random.binomial(1, 0.7, n)  # 70% in labor force\n",
    "\n",
    "# Non-linear Baseline Outcome for Income\n",
    "income_0 = baseline_income\n",
    "\n",
    "# More Non-linear Treatment Effect\n",
    "tau_income = (300 \n",
    "              + 50 * (X[:, 2])  # Linear effect for household size\n",
    "              + 20 * edu_level  # Linear term\n",
    "              + 10 * np.sin(edu_level)  # Non-linear term\n",
    "              + 5 * (X[:, 2] ** 2)  # Quadratic effect for household size\n",
    "              + 50 * np.log1p(X[:, 0])  # Non-linear effect for age group\n",
    "              + np.random.normal(0, 1500, n))  # Adding moderate noise\n",
    "\n",
    "# Treated outcome\n",
    "income_1 = income_0 + tau_income\n",
    "\n",
    "# Observed outcome\n",
    "income = W * income_1 + (1 - W) * income_0\n",
    "\n",
    "# Combine data into a DataFrame and rename it to data_sim3\n",
    "data_sim3 = pd.DataFrame({\n",
    "    'cohort': cohort,\n",
    "    'gender': gender,\n",
    "    'hh_size': hh_size,\n",
    "    'edu_level': edu_level,\n",
    "    'baseline_income': baseline_income,\n",
    "    'baseline_labor_force_status': baseline_labor_force_status,\n",
    "    'W': W,\n",
    "    'income': income\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data_sim3.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Average Bias: -0.7904896577587616\n",
      "Average MSE: 15536.79811099645\n",
      "Average RMSE: 124.64669314103945\n",
      "Average MAE: 99.57871754046364\n",
      "R-squared: 0.6168494995161107\n",
      "Explained Variance: 0.6184034536938602\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Average Bias: 0.7857682853255845\n",
      "Average MSE: 9975.267975490422\n",
      "Average RMSE: 99.87626332362672\n",
      "Average MAE: 79.4865747511864\n",
      "R-squared: 0.7537790391343141\n",
      "Explained Variance: 0.7551430669120281\n"
     ]
    }
   ],
   "source": [
    "# SIMULATION 3 RESULTS WITH RF AND GB REGRESSORS\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data_sim3[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((cohort_encoded, data_sim3[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "W_data = data_sim3['W'].values\n",
    "income_0_data = income_0\n",
    "income_1_data = income_1\n",
    "true_tau = tau_income\n",
    "\n",
    "# Define a function to perform a single iteration for Random Forest\n",
    "def single_iteration_rf(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Random Forest models\n",
    "    outcome_model_control = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    outcome_model_treated = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Define a function to perform a single iteration for Gradient Boosting\n",
    "def single_iteration_gb(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Gradient Boosting models\n",
    "    outcome_model_control = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10) #learning_rate=0.05)\n",
    "    outcome_model_treated = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)# learning_rate=0.05)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Parallelize the computation across multiple cores for Random Forest\n",
    "results_rf = Parallel(n_jobs=-1)(delayed(single_iteration_rf)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Random Forest\n",
    "avg_bias_rf = np.mean([result[0] for result in results_rf])\n",
    "avg_mse_rf = np.mean([result[1] for result in results_rf])\n",
    "avg_mae_rf = np.mean([result[2] for result in results_rf])\n",
    "avg_r_squared_rf = np.mean([result[3] for result in results_rf])\n",
    "avg_explained_variance_rf = np.mean([result[4] for result in results_rf])\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Average Bias: {avg_bias_rf}\")\n",
    "print(f\"Average MSE: {avg_mse_rf}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_rf)}\")\n",
    "print(f\"Average MAE: {avg_mae_rf}\")\n",
    "print(f\"R-squared: {avg_r_squared_rf}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_rf}\")\n",
    "\n",
    "# Parallelize the computation across multiple cores for Gradient Boosting\n",
    "results_gb = Parallel(n_jobs=-1)(delayed(single_iteration_gb)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Gradient Boosting\n",
    "avg_bias_gb = np.mean([result[0] for result in results_gb])\n",
    "avg_mse_gb = np.mean([result[1] for result in results_gb])\n",
    "avg_mae_gb = np.mean([result[2] for result in results_gb])\n",
    "avg_r_squared_gb = np.mean([result[3] for result in results_gb])\n",
    "avg_explained_variance_gb = np.mean([result[4] for result in results_gb])\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(f\"Average Bias: {avg_bias_gb}\")\n",
    "print(f\"Average MSE: {avg_mse_gb}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_gb)}\")\n",
    "print(f\"Average MAE: {avg_mae_gb}\")\n",
    "print(f\"R-squared: {avg_r_squared_gb}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cohort  gender  hh_size  edu_level  baseline_income   \n",
      "0  26-35       0        6  18.341193      1652.768920  \\\n",
      "1  56-66       0        2  16.273134      1891.797345   \n",
      "2  36-45       0        4  17.212297      1803.524148   \n",
      "3  36-45       1        7  17.874516      1493.089204   \n",
      "4   1-25       0       11  19.213854       618.861966   \n",
      "\n",
      "   baseline_labor_force_status  W       income  \n",
      "0                            0  1  2599.137878  \n",
      "1                            0  0  1891.797345  \n",
      "2                            1  1  2627.473208  \n",
      "3                            1  0  1493.089204  \n",
      "4                            1  0   618.861966  \n"
     ]
    }
   ],
   "source": [
    "# DGP for simulation 4 with ONLY linear terms in treatment effect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 10000  # Number of observations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Covariates\n",
    "# Age groups corresponding to cohorts\n",
    "age_groups = np.random.choice([1, 2, 3, 4, 5], size=n, p=[0.2, 0.3, 0.3, 0.1, 0.1])\n",
    "age_dict = {1: '1-25', 2: '26-35', 3: '36-45', 4: '46-55', 5: '56-66'}\n",
    "cohort = np.array([age_dict[age] for age in age_groups])\n",
    "\n",
    "gender = np.random.binomial(1, 0.5, n)\n",
    "hh_size = np.random.randint(1, 13, n)  # Household size varies from 1 to 12\n",
    "edu_level = np.random.normal(16, 2, n)\n",
    "\n",
    "# Covariate matrix (excluding cohort since it is categorical now)\n",
    "X = np.column_stack((age_groups, gender, hh_size, edu_level))\n",
    "\n",
    "# Desired proportion of treatment group\n",
    "treatment_proportion = 0.5  # 50% treatment, 50% control\n",
    "\n",
    "# Assign treatment based on a 50/50 probability distribution\n",
    "W = np.random.binomial(1, treatment_proportion, n)\n",
    "\n",
    "# Generate baseline income\n",
    "baseline_income = (1500 \n",
    "                   + 20 * X[:, 0]  # Linear term for age group (cohort)\n",
    "                   + 50 * X[:, 1]  # Linear effect for gender\n",
    "                   - 10 * X[:, 2]**2  # Quadratic term for hh_size\n",
    "                   + 25 * X[:, 3]  # Linear effect for edu_level\n",
    "                   + np.random.normal(0, 100, n))  # Add some noise\n",
    "\n",
    "# Generate baseline labor force status\n",
    "baseline_labor_force_status = np.random.binomial(1, 0.7, n)  # 70% in labor force\n",
    "\n",
    "# All linear vanilla treatment effect\n",
    "tau_income = (300 \n",
    "              + 50 * (X[:, 2])  # Linear effect for household size\n",
    "              + 20 * edu_level  # Linear effect for education level\n",
    "              + np.random.normal(0, 100, n))  # Adding minimal noise\n",
    "\n",
    "# Treated outcome\n",
    "income_1 = income_0 + tau_income\n",
    "\n",
    "# Observed outcome\n",
    "income = W * income_1 + (1 - W) * income_0\n",
    "\n",
    "# Combine data into a DataFrame and rename it to data_sim4\n",
    "data_sim4 = pd.DataFrame({\n",
    "    'cohort': cohort,\n",
    "    'gender': gender,\n",
    "    'hh_size': hh_size,\n",
    "    'edu_level': edu_level,\n",
    "    'baseline_income': baseline_income,\n",
    "    'baseline_labor_force_status': baseline_labor_force_status,\n",
    "    'W': W,\n",
    "    'income': income\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data_sim4.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Average Bias: 0.7899569959458497\n",
      "Average MSE: 11822.6325362386\n",
      "Average RMSE: 108.73192969978322\n",
      "Average MAE: 86.39272826540237\n",
      "R-squared: 0.7082312384800445\n",
      "Explained Variance: 0.7100623202177975\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Average Bias: 0.30331393987323746\n",
      "Average MSE: 9718.699758558865\n",
      "Average RMSE: 98.58346594920907\n",
      "Average MAE: 77.51552856619497\n",
      "R-squared: 0.7601316374955205\n",
      "Explained Variance: 0.7618391895129841\n"
     ]
    }
   ],
   "source": [
    "# Results from SIMULATION 4 WITH SF AND GB REGRESSORS\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data_sim4[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((cohort_encoded, data_sim4[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "W_data = data_sim4['W'].values\n",
    "income_0_data = income_0\n",
    "income_1_data = income_1\n",
    "true_tau = tau_income\n",
    "\n",
    "# Define a function to perform a single iteration for Random Forest\n",
    "def single_iteration_rf(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Random Forest models\n",
    "    outcome_model_control = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    outcome_model_treated = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Define a function to perform a single iteration for Gradient Boosting\n",
    "def single_iteration_gb(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    W_perm = np.random.permutation(W)\n",
    "    income_perm = income_1 * W_perm + income_0 * (1 - W_perm)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W_perm[sample_indices]\n",
    "    income_train = income_perm[sample_indices]\n",
    "\n",
    "    # Use Gradient Boosting models\n",
    "    outcome_model_control = GradientBoostingRegressor(n_estimators=100, max_depth=4, min_samples_leaf=10, learning_rate=0.05)\n",
    "    outcome_model_treated = GradientBoostingRegressor(n_estimators=100, max_depth=4, min_samples_leaf=10, learning_rate=0.05)\n",
    "    Xlearner = XLearner(models=(outcome_model_control, outcome_model_treated),\n",
    "                        propensity_model=LogisticRegression(max_iter=1000))\n",
    "    Xlearner.fit(income_train, W_train, X=X_train)\n",
    "    tau_hat = Xlearner.effect(X_train)\n",
    "\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Parallelize the computation across multiple cores for Random Forest\n",
    "results_rf = Parallel(n_jobs=-1)(delayed(single_iteration_rf)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Random Forest\n",
    "avg_bias_rf = np.mean([result[0] for result in results_rf])\n",
    "avg_mse_rf = np.mean([result[1] for result in results_rf])\n",
    "avg_mae_rf = np.mean([result[2] for result in results_rf])\n",
    "avg_r_squared_rf = np.mean([result[3] for result in results_rf])\n",
    "avg_explained_variance_rf = np.mean([result[4] for result in results_rf])\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Average Bias: {avg_bias_rf}\")\n",
    "print(f\"Average MSE: {avg_mse_rf}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_rf)}\")\n",
    "print(f\"Average MAE: {avg_mae_rf}\")\n",
    "print(f\"R-squared: {avg_r_squared_rf}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_rf}\")\n",
    "\n",
    "# Parallelize the computation across multiple cores for Gradient Boosting\n",
    "results_gb = Parallel(n_jobs=-1)(delayed(single_iteration_gb)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Gradient Boosting\n",
    "avg_bias_gb = np.mean([result[0] for result in results_gb])\n",
    "avg_mse_gb = np.mean([result[1] for result in results_gb])\n",
    "avg_mae_gb = np.mean([result[2] for result in results_gb])\n",
    "avg_r_squared_gb = np.mean([result[3] for result in results_gb])\n",
    "avg_explained_variance_gb = np.mean([result[4] for result in results_gb])\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(f\"Average Bias: {avg_bias_gb}\")\n",
    "print(f\"Average MSE: {avg_mse_gb}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_gb)}\")\n",
    "print(f\"Average MAE: {avg_mae_gb}\")\n",
    "print(f\"R-squared: {avg_r_squared_gb}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results with TLearner:\n",
      "Average Bias: -3.5925431837471753\n",
      "Average MSE: 33215.630561839105\n",
      "Average RMSE: 182.251558462031\n",
      "Average MAE: 141.11276330253747\n",
      "R-squared: 0.18645611815977609\n",
      "Explained Variance: 0.19511416162750442\n",
      "\n",
      "Gradient Boosting Results with TLearner:\n",
      "Average Bias: -4.131573423901446\n",
      "Average MSE: 11209.205225018752\n",
      "Average RMSE: 105.87353411036561\n",
      "Average MAE: 78.47263967474576\n",
      "R-squared: 0.7253406496064986\n",
      "Explained Variance: 0.7294113375554717\n"
     ]
    }
   ],
   "source": [
    "# Simulation 1 WITH T-LEARNER- random forests and gradient boosting base learners\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((cohort_encoded, data[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "W_data = data['W'].values\n",
    "income_0_data = income_0\n",
    "income_1_data = income_1\n",
    "true_tau = tau_income\n",
    "\n",
    "# Define a function to perform a single iteration for Random Forest using TLearner\n",
    "def single_iteration_rf_tlearner(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W[sample_indices]\n",
    "    income_train = income_1[sample_indices] * W_train + income_0[sample_indices] * (1 - W_train)\n",
    "\n",
    "    # Use Random Forest models\n",
    "    outcome_model = RandomForestRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10)\n",
    "    tlearner = TLearner(models=outcome_model)\n",
    "    tlearner.fit(Y=income_train, T=W_train, X=X_train)\n",
    "    tau_hat = tlearner.effect(X_train)\n",
    "\n",
    "    # Calculate metrics using the sampled portion of true_tau\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Define a function to perform a single iteration for Gradient Boosting using TLearner\n",
    "def single_iteration_gb_tlearner(X, W, income_0, income_1, true_tau, seed):\n",
    "    np.random.seed(seed)\n",
    "    sample_indices = np.random.choice(range(X.shape[0]), size=300, replace=False)\n",
    "    X_train = X[sample_indices]\n",
    "    W_train = W[sample_indices]\n",
    "    income_train = income_1[sample_indices] * W_train + income_0[sample_indices] * (1 - W_train)\n",
    "\n",
    "    # Use Gradient Boosting models\n",
    "    outcome_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=10, learning_rate=0.05)\n",
    "    tlearner = TLearner(models=outcome_model)\n",
    "    tlearner.fit(Y=income_train, T=W_train, X=X_train)\n",
    "    tau_hat = tlearner.effect(X_train)\n",
    "\n",
    "    # Calculate metrics using the sampled portion of true_tau\n",
    "    bias = np.mean(tau_hat - true_tau[sample_indices])\n",
    "    mse = np.mean((tau_hat - true_tau[sample_indices])**2)\n",
    "    mae = np.mean(np.abs(tau_hat - true_tau[sample_indices]))\n",
    "    ss_res = np.sum((tau_hat - true_tau[sample_indices])**2)\n",
    "    ss_tot = np.sum((true_tau[sample_indices] - np.mean(true_tau[sample_indices]))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    explained_variance = 1 - np.var(tau_hat - true_tau[sample_indices]) / np.var(true_tau[sample_indices])\n",
    "    \n",
    "    return bias, mse, mae, r_squared, explained_variance\n",
    "\n",
    "# Parallelize the computation across multiple cores for Random Forest using TLearner\n",
    "results_rf = Parallel(n_jobs=-1)(delayed(single_iteration_rf_tlearner)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Random Forest\n",
    "avg_bias_rf = np.mean([result[0] for result in results_rf])\n",
    "avg_mse_rf = np.mean([result[1] for result in results_rf])\n",
    "avg_mae_rf = np.mean([result[2] for result in results_rf])\n",
    "avg_r_squared_rf = np.mean([result[3] for result in results_rf])\n",
    "avg_explained_variance_rf = np.mean([result[4] for result in results_rf])\n",
    "\n",
    "print(\"Random Forest Results with TLearner:\")\n",
    "print(f\"Average Bias: {avg_bias_rf}\")\n",
    "print(f\"Average MSE: {avg_mse_rf}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_rf)}\")\n",
    "print(f\"Average MAE: {avg_mae_rf}\")\n",
    "print(f\"R-squared: {avg_r_squared_rf}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_rf}\")\n",
    "\n",
    "# Parallelize the computation across multiple cores for Gradient Boosting using TLearner\n",
    "results_gb = Parallel(n_jobs=-1)(delayed(single_iteration_gb_tlearner)(X_data, W_data, income_0_data, income_1_data, true_tau, i) for i in range(50))\n",
    "\n",
    "# Aggregate results for Gradient Boosting\n",
    "avg_bias_gb = np.mean([result[0] for result in results_gb])\n",
    "avg_mse_gb = np.mean([result[1] for result in results_gb])\n",
    "avg_mae_gb = np.mean([result[2] for result in results_gb])\n",
    "avg_r_squared_gb = np.mean([result[3] for result in results_gb])\n",
    "avg_explained_variance_gb = np.mean([result[4] for result in results_gb])\n",
    "\n",
    "print(\"\\nGradient Boosting Results with TLearner:\")\n",
    "print(f\"Average Bias: {avg_bias_gb}\")\n",
    "print(f\"Average MSE: {avg_mse_gb}\")\n",
    "print(f\"Average RMSE: {np.sqrt(avg_mse_gb)}\")\n",
    "print(f\"Average MAE: {avg_mae_gb}\")\n",
    "print(f\"R-squared: {avg_r_squared_gb}\")\n",
    "print(f\"Explained Variance: {avg_explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Average Bias: 41.53389767564463\n",
      "Average MSE: 1982648.61464069\n",
      "Average RMSE: 1408.065557650172\n",
      "Average MAE: 1012.2734478684324\n",
      "R-squared: 0.03290909371285766\n",
      "Explained Variance: 0.03375054102196817\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Average Bias: 26.725320310990405\n",
      "Average MSE: 1804728.8554881464\n",
      "Average RMSE: 1343.401970926106\n",
      "Average MAE: 971.7320164955144\n",
      "R-squared: 0.11969430610734244\n",
      "Explained Variance: 0.12004269747592466\n"
     ]
    }
   ],
   "source": [
    "# s LEARNER ON SIMULATION 1 WITH RANDOM FOREST AND GRADIENT BOOSTING REGRESSORS\n",
    "\n",
    "# One-hot encode the cohort categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "cohort_encoded = encoder.fit_transform(data[['cohort']])\n",
    "\n",
    "# Combine the encoded cohort with the rest of the features\n",
    "X_data = np.column_stack((\n",
    "    cohort_encoded, \n",
    "    data[['gender', 'hh_size', 'edu_level', 'baseline_income', 'baseline_labor_force_status']].values\n",
    "))\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "# The treatment variable\n",
    "W = data['W'].values\n",
    "\n",
    "# Observed income (the outcome)\n",
    "y = data['income'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, W_train, W_test = train_test_split(X_data, y, W, test_size=0.3, random_state=42)\n",
    "\n",
    "# Combine the features with the treatment for the S-learner\n",
    "X_train_with_treatment = np.column_stack((X_train, W_train))\n",
    "X_test_with_treatment = np.column_stack((X_test, W_test))\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_with_treatment, y_train)\n",
    "y_pred_rf = rf.predict(X_test_with_treatment)\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train_with_treatment, y_train)\n",
    "y_pred_gb = gb.predict(X_test_with_treatment)\n",
    "\n",
    "# Calculate performance metrics for Random Forest\n",
    "bias_rf = np.mean(y_pred_rf - y_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "explained_variance_rf = explained_variance_score(y_test, y_pred_rf)\n",
    "\n",
    "# Calculate performance metrics for Gradient Boosting\n",
    "bias_gb = np.mean(y_pred_gb - y_test)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mse_gb)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "explained_variance_gb = explained_variance_score(y_test, y_pred_gb)\n",
    "\n",
    "# Display results\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Average Bias: {bias_rf}\")\n",
    "print(f\"Average MSE: {mse_rf}\")\n",
    "print(f\"Average RMSE: {rmse_rf}\")\n",
    "print(f\"Average MAE: {mae_rf}\")\n",
    "print(f\"R-squared: {r2_rf}\")\n",
    "print(f\"Explained Variance: {explained_variance_rf}\")\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(f\"Average Bias: {bias_gb}\")\n",
    "print(f\"Average MSE: {mse_gb}\")\n",
    "print(f\"Average RMSE: {rmse_gb}\")\n",
    "print(f\"Average MAE: {mae_gb}\")\n",
    "print(f\"R-squared: {r2_gb}\")\n",
    "print(f\"Explained Variance: {explained_variance_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
